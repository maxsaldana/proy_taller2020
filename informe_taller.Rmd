---
title: "Informe preliminar"
author: "Valentina Caldiroli, Maximiliano Saldaña"
date: "16/4/2020"
output:
  pdf_document:
    toc: no
    pandoc_args: [
      "--number-sections",
      "--number-offset=1"
    ]
header-includes:
  - \usepackage{float}
  - \usepackage[spanish]{babel}

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=FALSE,
                      warning = FALSE,
                      results = FALSE,
                      message = FALSE,
                      fig.pos = 'H',
                      fig.align = 'center',
                      out.extra = '',
                      fig.hold = 'hold',
                      out.width = "50%"
                      )
options(xtable.comment=FALSE,
        xtable.table.placement="H")
```


```{r, include = FALSE}
library(tidyverse)
library(janitor)
library(xtable)
library(mfx)
```

\title{Modelización lineal del salario}
\author{Valentina Caldiroli, Maximiliano Saldaña}
\date{Noviembre 2020}



\maketitle



\newpage

\tableofcontents

\newpage








# Resumen ejecutivo




# Presentación del problema

El objetivo de este proyecto es estimar un modelo logístico mediante un método de máxima verosimilitud que aplique los métodos de optimización del curso y comparar el desempeño de nuestra estimación con el de las funciones base de R. Se hará uso de una base de salarios de Estados Unidos del año 1976, extraída del libro _Introducción a la Econometría, Un enfoque moderno (2009)_ de J.M. Wooldridge. La variable a ser explicada es una indicadora que vale 1 cuando el salario de la persona es mayor o igual del promedio y 0 cuando es menor. Las variables explicativas (que se eligieron en base a lo ya trabajado en el curso Modelos Lineales) serán el sexo de la persona, su educación, experiencia, antigüedad, región (dividida en Norte, Sur, Este y Oeste), si vive en una región metropolitana, rama de actividad (dividida en Construcción, Comercio, Servicios y Otros) y Ocupación (dividida en Profesional, Administrativos, Servicios y Otros).

El modelo en cuestión es de la forma siguiente _(Nalbarte, 2020)_:

<!-- Plantearlo asi en generico o particular? -->

$$P(Y_i = 1) = \frac{exp(x'_i\beta)}{1+exp(x'_i\beta)} = \pi_i = E(y_i)$$

Donde $x'_i\beta = \beta_0 + \beta_1 x_{i1} + \dots + \beta_1 x_{ip}$ (siendo $p$ el número de variables explicativas)

La función de unión (la que vincula $E(y_i)$ con $x'_i$) es:

$$log_e \left[\frac{\pi_i}{1-\pi_i}\right] = exp(x'_i\beta) $$

El problema de estimar el vector de parámetros $\beta$ se traduce en un problema de optimización, ya que buscamos que dicho vector sea el que maximice la función de verosimilitud de $Y$. Como $Y_i$ son variables aleatorias Bernoulli independientes e idénticamente distribuidas $\forall \,\, i=1,\dots,n$ la función de verosimilitud es:

$$L(\pi_1, \dots, \pi_n  | Y_1, \dots, Y_n) = \prod_{i=1}^n f_{Y_i}(y_i) = \prod_{i=1}^n \pi_i^{y_i}(1- \pi_i^{y_i})^{1-y_i}$$
Resulta equivalente trabajar con el logaritmo de la verosimilitud, que es:

$$log \,L(\pi_1, \dots, \pi_n  | Y_1, \dots, Y_n) = log \, \prod_{i=1}^n \pi_i^{y_i}(1- \pi_i^{y_i})^{1-y_i} = \sum_{i=1}^n \left[ Y_i log \left( \frac{\pi}{1+\pi} \right) \right] + \sum_{i=1}^n log(1-\pi_i)$$

Teniendo en cuenta que:

$$1-\pi_i = [1+exp(x'_i\beta)]^{-1}$$
$$\frac{\pi}{1+\pi} = exp(x'_i\beta)$$

$$log \, L(\beta|Y) = \sum_{i=1}^n  Y_i (x'_i\beta)   - \sum_{i=1}^n log(1+ exp(x'_i\beta)) $$

Estimaremos el vector $\beta$ que maximice la función anterior.


# Metodología usada

Para lograr optimizar la función de verosimilitud, se emplearán tres métodos estudiados en 
el curso, denominados: Ascenso más rápido, Método de Newton, y Método de quasi-Newton, o también conocido como BFGS.
El caso del primer método es utilizado en funciones contínuas con derivadas parciales de primer y segundo orden; su implementación se da a partir de una recursión dada por:
$$X_n+1 = X_n + \alpha_np_n  $$
De donde se denota que $\alpha_n$ es la longitud de paso con la que se realizará la búsqueda, y $p_n$ es la dirección de búsqueda, determinada por el vector gradiente, de modo que cuando la norma del gradiente es mayor a 0 la función asciende, y una vez que la norma es igual a 0 ese ascenso se detiene.

# Datos utilizados (Aplicación)

```{r, results='hide', echo = FALSE}
#Arreglo inicial de los datos

datos <- read_delim("wage1.txt", delim = "\t") %>% 
  as.data.frame %>% 
  clean_names() %>%
  rename(salario = "wage", reg.metro = "smsa", log.salario= "lwage")

as.numeric(sapply(datos, is.numeric))

names(subset(datos, select=!as.numeric(sapply(datos, is.numeric))))

sapply(subset(datos, select=!as.numeric(sapply(datos, is.numeric))), class)

head(subset(datos, select=!as.numeric(sapply(datos, is.numeric))))

datos <- datos %>%
  mutate(salario = gsub(",", ".", salario), log.salario = gsub(",", ".", log.salario) ) %>% 
  mutate_at(c("salario", "log.salario"), as.numeric)






#datos con variables indicadoras categoricas y hacemos categorica la variable salario (usamos las categorias menor a 15 y mayor a 15 dolares por hora)
datos <- datos %>% 
  mutate(
    region = as.factor(case_when(
               northcen == 1 ~ "northcen",
               south == 1 ~ "south",
               west == 1 ~ "west",
               northcen + south + west == 0 ~ "east"
               )),
    rama_act = as.factor(case_when(
                 construc == 1 ~ "construc",
                 trade == 1 ~ "trade",
                 services == 1 ~ "services",
                 construc + trade + services == 0 ~ "otros"
               )),
    ocupacion = as.factor(case_when(
                 profocc == 1 ~ "profocc",
                 clerocc == 1 ~ "clerocc",
                 servocc == 1 ~ "servocc",
                 profocc + clerocc + servocc == 0 ~ "otros",
    )),
    sal_med = ifelse(salario >= 5.896103 , 1, 0),
          
    educsq = educ^2
  ) %>% 
  dplyr::select(-profserv)






```

## Estimando mediante las funciones base

Inicialmente para tener como referencia y para comparar, estimaremos el modelo mediante la función de R _glm_.

```{r, echo = FALSE}
#modelo logit

modelo_ctrl <- glm(sal_med ~ female + educ + exper + tenure + northcen + south + west + reg.metro + construc + services + trade + profocc + clerocc + servocc, family = binomial (link = "logit") ,data = datos)


summary(modelo_ctrl)
```

Se puede observar que no todas las variables del conjunto resultan estadísticamente significativas.

```{r, echo=FALSE}
#comentar esto o eliminarlo?
mfx::logitor(sal_med ~ female + educ + exper + tenure + northcen + south + west + reg.metro + construc + services + trade + profocc + clerocc + servocc, data = datos)
```

## Estimación mediante los métodos vistos en clase

Retomando el planteo de nuestro problema, debemos buscar el máximo de nuestra función de verosimilitud, es decir:

$$	\max_\beta \,\, log \, L(\beta|Y) = \max_\beta \,\, \left[\sum_{i=1}^n  Y_i (x'_i\beta)   - \sum_{i=1}^n log(1+ exp(x'_i\beta))\right] $$

```{r, echo = FALSE}
#funcion a maximizar

X <- dplyr::select(datos,female, educ, exper, tenure, northcen, south, west, reg.metro, construc, services, trade, profocc, clerocc, servocc)

X <- cbind(rep(1, dim(X)[1]), X)

log_ver <- function(b){
  #b tiene que ser un vector k+1 = 14 + 1 dimensional
  
  lv <- sum((datos$sal_med)*(as.matrix(X)%*%b)) - sum(log(1-exp(as.matrix(X)%*%b))) 
  return(lv)
}

btest = rep(0.5,15)
```

Para emplear los métodos numéricos de optimización antes mencionados haremos uso de las funciones para aplicarlos vistas en el curso, las que fue necesario modificar para que fuera posible emplear aproximaciones numéricas del gradiente y la hessiana de la log-verosimilitud. 

### Ascenso más rápido

```{r, echo = FALSE}
##EXTRAIDO DE LO DE CLASE###

#para calcular numericamente el gradiente 
gradiente <- function(FUN, x0){
  
  f0 <- FUN(x0)
  h <- sqrt(.Machine$double.eps)
  k <- length(x0)
  dfx <- rep(0, k)
  
  for(i in 1:k){
    xi <- x0
    xi[i] <- xi[i] + h
    dfx[i] <- (FUN(xi)-f0)/h
  }
  
  return(dfx)
}

#hessiana
hessiana <- function(FUN, x0){
  
  f0 <- FUN(x0)
  h <- (.Machine$double.eps)^(1/3)
  k <- length(x0)
  Hfx <- matrix(0,k,k)
  
  for(i in 1:k){
    for(j in 1:k){
    xi <- x0
    xj <- x0
    xij <- x0
      
    xi[i] <- xi[i] + h
    xj[j] <- xj[j] + h
    
    xij[i] <- xij[i] + h
    xij[j] <- xij[j] + h

    Hfx[i,j] <- (FUN(xij)-FUN(xi)-FUN(xj)+f0)/(h^2)
    }
  }
  
  return(Hfx)
}




#busqueda lineal

blineal<-function(FUN,xn,pn,a_max=1,tau=0.5,maxiter_bt=10,c1=1e-4,c2=0.9,tau2=0.8,mostrar_bl=TRUE){
	phi<-function(x,a,p) FUN(x+a*p)
	phi0<-phi(xn,0,pn)
	alfa<-a_max
	phia<-phi(xn,alfa,pn)

	# condicion de ascenso suficiente
	iter_bt<-1
	armijo<-phi0+c1*alfa*sum(pn^2)
	while(phia<=armijo & iter_bt<maxiter_bt){
		alfa<-alfa*tau
		armijo<-phi0+alfa*c1*sum(pn^2)
		phia<-phi(xn,alfa,pn)
		iter_bt<-iter_bt+1
	}
	
  #condicion de curvatura
	iter_ft<-0
	while(sum(gradiente(FUN, xn+alfa*pn)*pn) > c2*sum(pn^2)){
		if(iter_ft==0 & mostrar_bl==TRUE) cat('Realizando forward-tracking','\n')
		iter_ft<-iter_ft+1
		alfa_old<-alfa/tau
		dif<-alfa_old - alfa
		alfa<-alfa + dif*(1-tau^iter_ft)
	}

	if(iter_bt==maxiter_bt & mostrar_bl==TRUE) cat('Fallo la busqueda lineal','\n')
	salida<-list(alfa=alfa,iter_bt=iter_bt,iter_ft=iter_ft)
	return(salida)
}




#metodo ascenso suficiente

ascenso <- function(FUN,x0,tol=1e-5,maxiter=1000,mostrar=TRUE,a_max=1,tau=0.5,maxiter_bt=10,c1=1e-4,c2=0.9,tau2=0.8,trace_bl=mostrar){
	pn <- gradiente(FUN, x0)
	alfa<-blineal(FUN,x0,pn,a_max=a_max,tau=tau,maxiter_bt=maxiter_bt,c1=c1,c2=c2,tau2=tau2,mostrar_bl=trace_bl)
	an<-alfa$alfa
	x1<-x0+an*pn
	iter<-1
	cambio<-sum(pn^2)

	while(cambio>tol & iter<maxiter){
		x0<-x1
		pn<-gradiente(FUN, x0)
		alfa<-blineal(FUN,x0,pn,a_max=a_max,tau=tau,maxiter_bt=maxiter_bt,c1=c1,c2=c2,tau2=tau2,mostrar_bl=trace_bl)
		an<-alfa$alfa
		x1<-x0+an*pn
		if(mostrar==TRUE) cat(paste('x',iter,sep=''),x1,'\n')
		iter<-iter+1
		cambio<-sum(pn^2)
	}
	if(iter==maxiter & mostrar==TRUE) cat('No se alcanzo convergencia','\n')

	salida<-list(x_max=x1,fx_max=FUN(x1),iter=iter)
	return(salida)
}
```



# Resultados (presentación y discusión)

# Conclusiones y futuros pasos


#Bibliografía





